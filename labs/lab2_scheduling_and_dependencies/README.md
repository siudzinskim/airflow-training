# Lab 2: Scheduling and Dependencies in Airflow

## Objective

This lab will teach you how to schedule DAGs and manage dependencies between tasks in Airflow. You will:

- Create a DAG that runs on a schedule.
- Use a Python script as a dependency within your DAG.
- Understand how to package dependencies with your DAG.

## Scenario

You need to build a data pipeline that continuously generates simulated e-commerce transaction data.  You have a Python script (`random_transactions.py`) that generates this data and writes it to a JSON file.  Your task is to create an Airflow DAG that executes this script on a schedule.

## Instructions

1. **Package `random_transactions.py`:**
   - Make sure `random_transactions.py` is in the same directory as your DAG file (`lab2_dag.py`). This is crucial for Airflow to find and execute the script correctly.  Airflow will automatically add the current directory to the Python path when searching for modules.

2. **Create the DAG (lab2_dag.py):**
   - Open the `lab2_dag.py` file.
   - Implement the DAG using the PythonOperator to execute the `generate_transactions` function from `random_transactions.py`.
   - Set the `schedule` parameter in the DAG to a desired schedule. For continuous execution (every minute), use `"@continuous"`.

3. **Set up dependencies:**
   - Add dependencies (if needed) between tasks in your DAG.  For example, if you have tasks to process the data generated by `random_transactions.py`, make them dependent on the task that runs the `random_transactions` script.

4. **Run the DAG:**
    - Upload your DAG to the `DAGS_FOLDER` or copy the current `labs` directory to the `DAGS_FOLDER`.
    - Go to the Airflow UI, find your DAG, and turn it on.

5. **Monitor execution:**
   - Verify in the Airflow UI that the DAG runs on the schedule you defined.
   - Check logs to confirm that transactions data is being generated.



## Tips

- The `faker` python library is not available by default we need to add it along with `lab2_dad.py` and `random_transactions.py` 
- **NOTE:** The syntax using decorators require a function invocation. After pushing `lab2_dag.py` to `dags` the DAG will not be parsed. Uncomment the function invoction.
- The `dest_file_prefix` is required - set the parameter and 
- The `data` folder mounted in Airflow workers needs to have `777` umask set. In case of error: `mkdir: cannot create directory ‘/opt/airflow/data/transactions’: Permission denied` change the directory permissions.

## Bonus Challenge
- Package required files with `faker` into a single `zip` archive to keep cleaner `dags` folder.