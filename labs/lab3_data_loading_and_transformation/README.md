# Lab 3: Data Loading and Transformation with DuckDB and Airflow

## Objective

This lab guides you through loading data from CSV and newline-delimited JSON files into a DuckDB database, performing a join operation, and writing the results to a new newline-delimited JSON file using Airflow.

## Scenario

You have customer data in `customers.csv` (generated by `random_customer.py` script) and transaction data in multiple `transactions_*.json` files (generated in Lab 2).  Your task is to build an Airflow DAG that:

1. Loads the customer and transaction data into a DuckDB database.
2. Joins the two tables on the `customer_id` column.
3. Writes the joined data to a new set of `full_transaction_*.json` files (newline-delimited JSON), one file per original transaction file.

## Instructions

1. **Set up the environment:**
   - Ensure you have the `duckdb` Python package installed in your Airflow environment.  You can add it to your `requirements.txt` file (if you're using one) or install it directly:
     ```bash
     pip install duckdb
     ```

2. **Prepare the data:**
    - Run the `random_customer.py` script from the `generators` directory to generate or update the `customers.csv` data. Make sure this data is copied to the location accessible by your airflow workers (e.g. if you are using shared volume - place it there).
    -  Ensure that the `transactions_*.json` files generated in Lab 2 are available in the location accessed by the airflow workers.

3. **Create the DAG (`lab3_dag.py`):**
   - Implement the DAG using the provided template. The DAG should contain a `PythonOperator` that performs the following:
      - Connects to a DuckDB database (in-memory is sufficient for this lab).
      - Creates the `customers` and `transactions` tables in DuckDB if they don't exist (ensure the schema matches your data).
      - Loads data from `customers.csv` into the `customers` table using `COPY`.
      - Iterates through the available `transactions_*.json` files. For each file:
        - Loads data from the current JSON file into the `transactions` table.
        - Performs a JOIN operation between the `customers` and `transactions` tables on the `customer_id`.
        - Writes the joined data to a corresponding `full_transactions_*.json` file in newline-delimited JSON format.

4. **Configure the DAG:**
    - Set an appropriate schedule (e.g., `@hourly`, `@daily`, or `@continuous` if you want it to run continuously).
    - Set the `data_dir` parameter in the DAG to the correct location of your data files accessible to Airflow workers, e.g. `/opt/airflow/data`.

5. **Run the DAG:**
    - Copy `lab3_dag.py` to your Airflow DAGs folder.
    - Trigger the DAG (manually or let it run according to the schedule).


6. **Verify the output:**
   - Confirm that the `full_transactions_*.json` files are being created in the specified output directory and that the files contain the correctly joined data.

## Tips:

- Handle potential errors during file processing and database operations.  Add error handling and logging to your Python code.
- For dynamic SQL queries, consider using a templating engine like Jinja.

